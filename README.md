# demystifying-machine-learning-2022

# Daily Github Pull

Each day we will pull the most recent version of the daily jupyter notebook from this central Github location. Don't get ahead of yourself -- we're tired overworked grad students and will likely be uploading the final version the night before or morning of class. So we'll perform this pull every day together.

### First day
1) open terminal or other application for the bash/zsh command line 
2) `cd` into the directory you want your bootcamp materials to live in
3) `git clone https://github.com/NRT-DSEER/demystifying-machine-learning-2022`
4) `cd demystifying-machine-learning-2022`
5) `cd notebooks`
6) `jupyter notebook`

### Subsequent days
1) open terminal
2) move into bootcamp directory
3) `git pull`
4) `cd notebooks`
5) `jupyter notebook`

# Syllabus

### Instructors:
All the instructors are also on Slack, we prefer that you send us a slack message if you have any questions over sending an email. Slack is only available for students taking the live version of the course. Please check your email for information on how to join slack. [This video](https://www.youtube.com/watch?v=Xm790AkFeK4&ab_channel=TraversyMedia) and others like it may be helpful.

- Jordan Fuhrman, PhD student in Medical Physics
- Samantha Lapp, PhD student in Department of Physics + Department of Geophysical Sciences

### Teaching Assistants:
Contact over slack or attend office hours (the final 30 minutes of each day, over zoom or in person)

- Owen Melia, PhD student in Computer Science
- Matthew Schmitt, PhD student in Physics
- Shrikanth Subramanian, 2nd year MS student in Computer Science
- Salman Yousaf, incoming MS student in the Analytics program

### Class Time:
- Sept 15th - Sept 16th
- 09:30am-12:30pm CT 


-Zoom link is bookmarked at top of the machine_learning slack channel


### Recordings:
The folder for the recordings will be posted over Slack or sent over email. We will add each day's recordings a few hours after classtime.

## Day 1: Random Forest

- splitting data into training and test/validation sets
- overfitting/underfitting
- binary splits
- decision trees
- hyperparameter tuning (using k-fold cross validation via grid or random search)
- bagging ( = bootstrapping + aggregation)
- putting it all together: random forest


## Day 2: Neural Networks

-machine learning vs. deep learning
-fundamentals neural network structure
-supervised learning
-loss functions and gradient descent
-image classification
