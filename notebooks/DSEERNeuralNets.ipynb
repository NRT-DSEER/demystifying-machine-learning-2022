{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec522768",
   "metadata": {},
   "source": [
    "Welcome to the NRT-DSEER Lecture: Demystifying Machine Learning: Neural Networks course! \n",
    "\n",
    "This lecture is intended to provide a practical introduction to neural networks including fundamental design, the learning process, and common applications.  The goal is to gain a sufficient understanding to follow discussion of neural networks in seminars, meetings, etc.  This lecture does not provide an in-depth look at the mathematics and other more technical aspects of neural networks; if you plan to implement neural networks in your own research, I highly encourage you to utilize additional resources/references to gain a more thorough understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c6e1e",
   "metadata": {},
   "source": [
    "\n",
    "The framework utilized for this lecture is TensorFlow with the Keras API. This is one of two primary deep learning frameworks utilized in AI research (Tensorflow and PyTorch), although there are others and popularity may vary depending on your field. More information about TensorFlow and Keras can be found here: https://www.tensorflow.org/  https://keras.io/\n",
    "\n",
    "We need to first install the tensorflow package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96771b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Model, activations, optimizers, metrics\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "#from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d402d93",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We'll begin by experimenting with some simple neural networks to understand basic design parameters.\n",
    "\n",
    "Let's work on a binary classification problem in which we have two Gaussian distributions and we would like to predict which distribution an unseen data point is likely to .  To start, we need to generate some fake data to use for our task.  We will need training, validation, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1890) #establish a random seed to make code/results reproducible\n",
    "\n",
    "mu1 = [1, 1]\n",
    "sig1 = [[1.5, 0.2], [0.2, 1.5]]\n",
    "\n",
    "mu2 = [5, 2]\n",
    "sig2 = [[1.5, -1], [-1, 4]]\n",
    "\n",
    "data1 = np.random.multivariate_normal(mu1, sig1, size=500)\n",
    "data2 = np.random.multivariate_normal(mu2, sig2, size=500)\n",
    "train_data = np.concatenate((data1,data2))\n",
    "\n",
    "data1 = np.random.multivariate_normal(mu1, sig1, size=50)\n",
    "data2 = np.random.multivariate_normal(mu2, sig2, size=50)\n",
    "validation_data = np.concatenate((data1,data2))\n",
    "\n",
    "data1 = np.random.multivariate_normal(mu1, sig1, size=100)\n",
    "data2 = np.random.multivariate_normal(mu2, sig2, size=100)\n",
    "test_data = np.concatenate((data1,data2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb22be",
   "metadata": {},
   "source": [
    "We can visualize the generated data to ensure that it is appropriate for our binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c1df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.concatenate((np.zeros(500,), np.ones(500,)))\n",
    "validation_labels = np.concatenate((np.zeros(50,), np.ones(50,)))\n",
    "test_labels = np.concatenate((np.zeros(100,), np.ones(100,)))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "axs[0].scatter(train_data[:,0],train_data[:,1], c=train_labels)\n",
    "axs[0].set_title('Train Data')\n",
    "axs[1].scatter(validation_data[:,0],validation_data[:,1], c=validation_labels)\n",
    "axs[1].set_title('Validation Data')\n",
    "axs[2].scatter(test_data[:,0],test_data[:,1], c=test_labels)\n",
    "axs[2].set_title('Test Data')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim(-3,9)\n",
    "    ax.set_ylim(-5,8)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f210f9",
   "metadata": {},
   "source": [
    "Now we can get to deep learning.  The typical workflow is:\n",
    "1) Generate the neural network architecture. \n",
    "2) Compile the model and provide training strategy. \n",
    "3) Train the model \n",
    "4) Evaluate performance\n",
    "\n",
    "Keras provides an intuitive approach to building models.  Let's start by defining the model architecutre/parameters (you can use this link to view the activation function options: https://keras.io/api/layers/activations/#about-advanced-activation-layers)\n",
    "\n",
    "Syntax of Dense layers: LayerType(number_of_nodes)(previous_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc29d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'ModelWeights.h5' #location/file name for model weights as .h5 file\n",
    "input_size = [2]\n",
    "num_layers =  ?  #the number of HIDDEN layers in the model\n",
    "num_nodes = ?   #the number of nodes/neurons in each hidden layer\n",
    "\n",
    "\n",
    "def get_classification_model(input_size, num_layers, num_nodes):\n",
    "    \n",
    "    input_x = Input((input_size), name='input')\n",
    "    hlayer = Dense(num_nodes)(input_x)\n",
    "    activation = ? #insert activation layer here\n",
    "    \n",
    "    for i_layers in range(num_layers-1):\n",
    "        hlayer = Dense(num_nodes)(activation)\n",
    "        activation = ? #insert activation layer here\n",
    "    \n",
    "    output_y = Dense(1, activation='sigmoid')(hlayer)\n",
    "    \n",
    "    model = Model(inputs=input_x, outputs=output_y)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_classification_model(input_size, num_layers, num_nodes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e78f55",
   "metadata": {},
   "source": [
    "Step 2 is to compile the model and provide training protocol (we'll learn more about this in section 2, so don't worry about changing anything here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.SGD(), loss='binary_crossentropy', metrics = [\"accuracy\"])\n",
    "\n",
    "callbacks = [EarlyStopping(patience=20, verbose=1),\n",
    "            ReduceLROnPlateau(factor=0.1, patience=12, min_lr=1e-7, verbose=1),\n",
    "            ModelCheckpoint(save_path, verbose=1, save_best_only=True, save_weights_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e1c49",
   "metadata": {},
   "source": [
    "Finally, we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=model.fit(train_data, train_labels, batch_size=1, epochs=30, validation_data=(validation_data,validation_labels), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9bf3c",
   "metadata": {},
   "source": [
    "And use the testing data to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81474ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_data)\n",
    "test_predictions = (test_predictions>0.5).astype(np.uint8)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score((test_labels).astype(np.uint8), test_predictions)\n",
    "print('Model accuracy is ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b96a94",
   "metadata": {},
   "source": [
    "We can also create dummy data to help us visualize the decision boundary with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dummy = np.arange(-3,9,0.05)\n",
    "y_dummy = np.arange(-5,8,0.05)\n",
    "dummy_data = np.array(np.meshgrid(x_dummy,y_dummy)).T.reshape(-1,2)\n",
    "\n",
    "dummy_predictions = model.predict(dummy_data)\n",
    "dummy_predictions = (dummy_predictions>0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array(['red','green'])\n",
    "plt.scatter(dummy_data[:,0],dummy_data[:,1],c=dummy_predictions, cmap='RdYlGn')\n",
    "plt.scatter(test_data[:,0],test_data[:,1], c=test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f26d77",
   "metadata": {},
   "source": [
    "STOP HERE UNTIL INSTRUCTED TO MOVE ON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d97e8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Exercise: Using the code above as a template, create a new model to either 1) perform classification on the Iris Flower dataset or 2) perform regression on the Wine Quality dataset. Do not worry about data visualization, scatter plots, etc.  Think about which model parameters and concepts need to be adjusted from above to prep your data, design your model, train, and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv('../data/iris.data', names=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Class'])\n",
    "#or\n",
    "#pd.read_csv('../data/winequality-white.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83651e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4ac28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d6f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('The RMSE of the wine quality is ', )\n",
    "#or\n",
    "#print('The accuracy(?) of the flower classifier is ', )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a14870a",
   "metadata": {},
   "source": [
    "STOP HERE UNTIL INSTRUCTED TO MOVE ON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba50a0",
   "metadata": {},
   "source": [
    "We will now start to investigate different loss functions.  \n",
    "At this link, find a list of loss functions available through Keras: https://keras.io/api/losses/\n",
    "And the list of available optimizers given here: https://keras.io/api/optimizers/\n",
    "\n",
    "Exercise 1: Write the loss function to calculate the mean absolute error (MAE) loss.\n",
    "\n",
    "$ MAE = \\sum \\limits _{i=1} ^{N}\\frac{|y_{i}-\\hat y_{i}|}{N} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf59dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAELoss(y_true, y_pred):\n",
    "    #y_true is a vector of ground truth values\n",
    "    #y_pred is a vector of data predictions\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13078953",
   "metadata": {},
   "source": [
    "Exercise 2: Consider a more complex loss function in which we want to evaluate both the MAE and the MSE (mean squared error), but more heavily weight the MSE? Write a loss function that can take weighting parameters as inputs for the MAE and MSE. Try utilizing this loss to train the regression model you wrote above.\n",
    "\n",
    "$ MSE = \\sum \\limits _{i=1} ^{N}\\frac{(y_{i}-\\hat y_{i})^{2}}{N} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_MSE_MAE_loss(y_true, y_pred, weightMSE, weightMAE):\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c161e",
   "metadata": {},
   "source": [
    "STOP HERE UNTIL INSTRUCTED TO MOVE ON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c6915",
   "metadata": {},
   "source": [
    "Finally, we will have a brief demonstraion of how we can utilize a convolutional neural network for a more complex task: image classification.  We begin by loading in an image and reformatting it to the image size specified by the model architecture (224 pixels by 224 pixels).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d6d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_img('../data/cat_image.jpg', target_size=(224,2224))\n",
    "image = img_to_array(image)\n",
    "image = np.expand_dims(image, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a7a64",
   "metadata": {},
   "source": [
    "Now, let's load a pre-trained image classification model provided by Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG19(include_top=True, weights='imagenet')\n",
    "y_image = model.predict(image)\n",
    "print(np.shape(y_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdf669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
